#!/usr/bin/env python3
"""
è¯­ä¹‰æœç´¢è„šæœ¬

ä½¿ç”¨åµŒå…¥å‘é‡è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
"""

import sys
import re
from pathlib import Path
from datetime import datetime
import yaml
import math


def parse_frontmatter(content):
    """
    è§£æ YAML frontmatter

    Args:
        content: æ–‡ä»¶å†…å®¹

    Returns:
        (frontmatter, body)
    """
    if content.startswith('---'):
        parts = content.split('---', 2)
        if len(parts) >= 2:
            frontmatter_text = parts[1]
            body = parts[2] if len(parts) > 2 else ''
            try:
                frontmatter = yaml.safe_load(frontmatter_text)
                return frontmatter, body
            except:
                return {}, body
    return {}, content


def cosine_similarity(vec1, vec2):
    """
    è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦

    Args:
        vec1: å‘é‡1
        vec2: å‘é‡2

    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    if not vec1 or not vec2:
        return 0.0

    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude1 = math.sqrt(sum(a * a for a in vec1))
    magnitude2 = math.sqrt(sum(b * b for b in vec2))

    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0

    return dot_product / (magnitude1 * magnitude2)


def generate_simple_embedding(text):
    """
    ç”Ÿæˆç®€å•çš„æ–‡æœ¬åµŒå…¥ï¼ˆåŸºäºTF-IDFçš„ç®€åŒ–ç‰ˆæœ¬ï¼‰

    æ³¨æ„: è¿™æ˜¯ç®€åŒ–å®ç°ã€‚ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ä¸“ä¸šçš„åµŒå…¥æ¨¡å‹
    å¦‚ sentence-transformers, OpenAI embeddings, ç­‰

    Args:
        text: è¾“å…¥æ–‡æœ¬

    Returns:
        åµŒå…¥å‘é‡
    """
    # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
    words = re.findall(r'\w+', text.lower())

    if not words:
        return []

    # è®¡ç®—è¯é¢‘
    word_freq = {}
    for word in words:
        word_freq[word] = word_freq.get(word, 0) + 1

    # ç”Ÿæˆå‘é‡ï¼ˆä½¿ç”¨å›ºå®šå¤§å°çš„è¯æ±‡è¡¨ï¼‰
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ä½¿ç”¨é¢„è®­ç»ƒçš„åµŒå…¥æ¨¡å‹
    vocab_size = 1000
    embedding = [0.0] * vocab_size

    for word, freq in word_freq.items():
        # ä½¿ç”¨ç®€å•çš„å“ˆå¸Œå‡½æ•°å°†è¯æ˜ å°„åˆ°ç´¢å¼•
        word_hash = hash(word) % vocab_size
        embedding[word_hash] = float(freq)

    # å½’ä¸€åŒ–
    magnitude = math.sqrt(sum(v * v for v in embedding))
    if magnitude > 0:
        embedding = [v / magnitude for v in embedding]

    return embedding


def semantic_search(query, database_path, threshold=0.7, filters=None):
    """
    è¯­ä¹‰æœç´¢

    Args:
        query: æœç´¢æŸ¥è¯¢
        database_path: æ•°æ®åº“è·¯å¾„
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        filters: è¿‡æ»¤æ¡ä»¶

    Returns:
        ç»“æœåˆ—è¡¨ [(record, similarity), ...]
    """
    vault = Path(database_path)
    memory_folder = vault / "memory"

    if not memory_folder.exists():
        print("âŒ é”™è¯¯: memory æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        return []

    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_vector = generate_simple_embedding(query)

    results = []

    # éå†æ‰€æœ‰è®°å¿†æ–‡ä»¶
    for md_file in memory_folder.glob("*.md"):
        try:
            content = md_file.read_text(encoding="utf-8")
            frontmatter, body = parse_frontmatter(content)

            # ç”Ÿæˆè®°å½•å‘é‡ï¼ˆæ ‡é¢˜ + å†…å®¹ï¼‰
            text_to_embed = frontmatter.get("title", "") + " " + body[:500]
            record_vector = generate_simple_embedding(text_to_embed)

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = cosine_similarity(query_vector, record_vector)

            # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œæ·»åŠ åˆ°ç»“æœ
            if similarity >= threshold:
                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if filters:
                    if "date" in filters:
                        record_date = frontmatter.get("date", "")
                        if record_date != filters["date"]:
                            continue
                    if "type" in filters:
                        record_type = frontmatter.get("type", "")
                        if record_type != filters["type"]:
                            continue
                    if "importance" in filters:
                        record_importance = frontmatter.get("importance", 0)
                        if record_importance < filters["importance"]:
                            continue

                record = {
                    "file": md_file,
                    "frontmatter": frontmatter,
                    "body": body,
                    "similarity": similarity
                }
                results.append(record)

        except Exception as e:
            print(f"âš ï¸  è·³è¿‡æ–‡ä»¶ {md_file}: {e}")
            continue

    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    results.sort(key=lambda x: x["similarity"], reverse=True)

    return results


def display_results(results, query, max_results=10):
    """
    æ˜¾ç¤ºè¯­ä¹‰æœç´¢ç»“æœ

    Args:
        results: ç»“æœåˆ—è¡¨
        query: æœç´¢æŸ¥è¯¢
        max_results: æœ€å¤§æ˜¾ç¤ºæ•°é‡
    """
    if not results:
        print(f"\nğŸ” æœªæ‰¾åˆ°ä¸ \"{query}\" è¯­ä¹‰ç›¸å…³çš„è®°å½•")
        print("\nå»ºè®®:")
        print("  - å°è¯•æ›´å…·ä½“çš„æŸ¥è¯¢")
        print("  - é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼")
        print("  - ä½¿ç”¨å…³é”®è¯æœç´¢: /search [å…³é”®è¯]")
        return

    print(f"\nğŸ” è¯­ä¹‰æœç´¢: \"{query}\"")
    print(f"æ‰¾åˆ° {len(results)} æ¡ç›¸å…³è®°å½•\n")

    display_count = min(len(results), max_results)

    for i, record in enumerate(results[:display_count], 1):
        fm = record["frontmatter"]
        similarity = record["similarity"]
        stars = "â­" * fm.get("importance", 3)
        similarity_percent = int(similarity * 100)

        print(f"[{i}] {stars} {fm.get('title', 'æ— æ ‡é¢˜')}")
        print(f"    ğŸ“… {fm.get('date', '')} | {fm.get('time', '')}")
        print(f"    ğŸ·ï¸ {' '.join(fm.get('tags', []))}")
        print(f"    ğŸ”— é¡¹ç›®: {fm.get('project', 'æœªæŒ‡å®š')}")
        print(f"    ğŸ“Š çŠ¶æ€: {fm.get('status', '')}")
        print(f"    ğŸ’¡ ç›¸ä¼¼åº¦: {similarity_percent}%")
        print(f"    ğŸ“ æ–‡ä»¶: {record['file'].name}")

        # æå–æ‘˜è¦
        body = record["body"]
        if len(body) > 100:
            snippet = body[:100]
            print(f"    ğŸ“„ æ‘˜è¦: {snippet}...")

        print()

    if len(results) > max_results:
        print(f"â„¹ï¸  è¿˜æœ‰ {len(results) - max_results} æ¡è®°å½•æœªæ˜¾ç¤º")


if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python semantic_search.py <ç¬”è®°åº“è·¯å¾„> <æŸ¥è¯¢> [é€‰é¡¹]")
        print("\né€‰é¡¹:")
        print("  --threshold <æ•°å­—>    è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1, é»˜è®¤: 0.7)")
        print("  --type <ç±»å‹>         ä»…æœç´¢æŒ‡å®šç±»å‹")
        print("  --date <æ—¥æœŸ>         ä»…æœç´¢æŒ‡å®šæ—¥æœŸ")
        print("  --importance <æ•°å­—>   ä»…æœç´¢é‡è¦ç¨‹åº¦>=Nçš„è®°å½•")
        print("  --max <æ•°å­—>          æœ€å¤šæ˜¾ç¤ºNæ¡ç»“æœ")
        print("\nç¤ºä¾‹:")
        print("  python semantic_search.py ~/Obsidian/Vault \"æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡æ•°æ®åº“å—ï¼Ÿ\"")
        print("  python semantic_search.py ~/Obsidian/Vault æŠ€æœ¯å†³ç­– --threshold 0.6")
        print("  python semantic_search.py ~/Obsidian/Vault API --type decision")
        sys.exit(1)

    vault_path = sys.argv[1]
    query = sys.argv[2]

    # è§£æé€‰é¡¹
    threshold = 0.7
    filters = {}
    max_results = 10

    i = 3
    while i < len(sys.argv):
        arg = sys.argv[i]

        if arg == "--threshold" and i + 1 < len(sys.argv):
            threshold = float(sys.argv[i + 1])
            i += 2
        elif arg == "--type" and i + 1 < len(sys.argv):
            filters["type"] = sys.argv[i + 1]
            i += 2
        elif arg == "--date" and i + 1 < len(sys.argv):
            filters["date"] = sys.argv[i + 1]
            i += 2
        elif arg == "--importance" and i + 1 < len(sys.argv):
            filters["importance"] = int(sys.argv[i + 1])
            i += 2
        elif arg == "--max" and i + 1 < len(sys.argv):
            max_results = int(sys.argv[i + 1])
            i += 2
        else:
            i += 1

    # æ‰§è¡Œæœç´¢
    results = semantic_search(query, vault_path, threshold, filters)

    # æ˜¾ç¤ºç»“æœ
    display_results(results, query, max_results)

    print("\nâš ï¸  æ³¨æ„: å½“å‰ä½¿ç”¨ç®€åŒ–çš„åµŒå…¥ç®—æ³•")
    print("å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ä¸“ä¸šçš„åµŒå…¥æ¨¡å‹:")
    print("  - sentence-transformers (Hugging Face)")
    print("  - OpenAI Embeddings API")
    print("  - Cohere Embeddings")
